{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Function to load .mat files\n",
    "def load_mat_file(file_path):\n",
    "    return scipy.io.loadmat(file_path)\n",
    "\n",
    "# Function to print basic data info\n",
    "def basic_data_info(expression_df, labels_df):\n",
    "    print(f\"Number of samples (rows): {expression_df.shape[0]}\")\n",
    "    print(f\"Number of genes (columns): {expression_df.shape[1]}\")\n",
    "    print(\"\\nLabels information:\")\n",
    "    print(labels_df.value_counts())\n",
    "\n",
    "# Function to plot confusion matrix with cell names like TP, TN, FP, FN\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    labels = np.array([[\"TN\", \"FP\"], [\"FN\", \"TP\"]])\n",
    "    cm_with_labels = np.array([f\"{label}\\n{cm_val}\" for label, cm_val in zip(labels.flatten(), cm.flatten())])\n",
    "    cm_with_labels = cm_with_labels.reshape(2, 2)\n",
    "    \n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(cm, annot=cm_with_labels, fmt=\"\", cmap='Blues', cbar=False, square=True, \n",
    "                xticklabels=[\"Predicted 0\", \"Predicted 1\"], yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "# Function to split data into train and test sets\n",
    "def split_data(expression_df, labels_df, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(expression_df, labels_df, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to perform grid search\n",
    "def perform_grid_search(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n",
    "\n",
    "# Function to train the model\n",
    "def train_best_model(grid_search, X_train, y_train):\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    return best_model\n",
    "\n",
    "# Function to calculate node accuracy\n",
    "def calculate_node_accuracy(tree_model, X_train, y_train):\n",
    "    # Get the leaf node IDs for each sample in the training data\n",
    "    leaf_ids = tree_model.apply(X_train)\n",
    "\n",
    "    # Dictionary to store node accuracy information\n",
    "    node_accuracies = {}\n",
    "\n",
    "    # Iterate over all unique leaf nodes\n",
    "    for node_id in np.unique(leaf_ids):\n",
    "        # Get samples in the leaf node (use positional indices)\n",
    "        sample_ids = np.where(leaf_ids == node_id)[0]  # These are positional indices\n",
    "\n",
    "        # Use iloc to access rows by positional index\n",
    "        y_true = y_train.iloc[sample_ids]  \n",
    "        y_pred = tree_model.predict(X_train.iloc[sample_ids])  # Make sure X_train is a DataFrame\n",
    "\n",
    "        # Calculate node accuracy (correct predictions over total samples)\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        total_samples = len(sample_ids)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        # Store node accuracy\n",
    "        node_accuracies[node_id] = accuracy\n",
    "\n",
    "        # Print debugging information\n",
    "        # print(f\"Node {node_id}: Total samples = {total_samples}, Correct predictions = {correct_predictions}, Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "    return node_accuracies\n",
    "\n",
    "# Function to visualize the decision tree with node accuracy and node IDs\n",
    "def visualize_tree_with_node_accuracy(tree_model, node_accuracies, feature_names):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    tree_plot = tree.plot_tree(tree_model, filled=True, feature_names=feature_names, class_names=[\"0\", \"1\"], rounded=True)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Annotate each node with its node ID and accuracy if it's a leaf\n",
    "    for idx, text in enumerate(ax.texts):\n",
    "        node_id = idx  # Node index corresponds to the position in the plot\n",
    "        # Display the node ID on the graph\n",
    "        text.set_text(f\"Node {node_id}\\n\" + text.get_text())\n",
    "        \n",
    "        # If the node is a leaf node, append the accuracy\n",
    "        if node_id in node_accuracies:\n",
    "            node_accuracy = node_accuracies[node_id]\n",
    "            updated_text = text.get_text() + f\"\\nAccuracy: {node_accuracy:.2f}\"\n",
    "            text.set_text(updated_text)\n",
    "\n",
    "    plt.title(\"Decision Tree Visualization with Node IDs and Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(best_model, X_test, y_test):\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "base_path = '/Users/asifahmed/Documents/Codes/MyRecourseProject/datasets/ACES_dataset'\n",
    "\n",
    "# Load the expression data and labels\n",
    "expression_data = load_mat_file(f'{base_path}/ACES_RefinedCommunity_AVG.mat')\n",
    "label_data = load_mat_file(f'{base_path}/ACESLabel.mat')\n",
    "\n",
    "# Extract the expression matrix and labels\n",
    "expression_matrix = expression_data['data']\n",
    "labels = label_data['label'].flatten()  # Flatten to a 1D array if necessary\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "expression_df = pd.DataFrame(expression_matrix)\n",
    "labels_df = pd.Series(labels, name='label')\n",
    "\n",
    "# Display basic data information\n",
    "basic_data_info(expression_df, labels_df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = split_data(expression_df, labels_df)\n",
    "\n",
    "# Train the decision tree using GridSearch\n",
    "grid_search = perform_grid_search(X_train, y_train)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = train_best_model(grid_search, X_train, y_train)\n",
    "\n",
    "# Evaluate the initial model\n",
    "print(\"\\nInitial Model Evaluation:\")\n",
    "evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "# Calculate node accuracy\n",
    "node_accuracies = calculate_node_accuracy(best_model, X_train, y_train)\n",
    "\n",
    "# Visualize the initial tree with node accuracy\n",
    "print(\"\\nInitial Decision Tree Visualization with Node Accuracy:\")\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = [f\"x{index}\" for index in range(num_features)]\n",
    "visualize_tree_with_node_accuracy(best_model, node_accuracies, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_feature_ranges(tree_model, X_train, y_train, node_ids, features):\n",
    "    ranges = {}\n",
    "    for node_id in node_ids:\n",
    "        leaf_ids = tree_model.apply(X_train)\n",
    "        samples_in_node = X_train.iloc[leaf_ids == node_id]\n",
    "        class_1_samples = samples_in_node[y_train.iloc[leaf_ids == node_id] == 1]\n",
    "        ranges[node_id] = {feature: (class_1_samples.iloc[:, feature].min(), class_1_samples.iloc[:, feature].max()) for feature in features}\n",
    "    return ranges\n",
    "\n",
    "def plot_feature_ranges(ranges, features):\n",
    "    colors = [\"green\", \"red\"]  # Colors for the nodes\n",
    "    fig, axs = plt.subplots(len(features), 1, figsize=(13, len(features) * 2))\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        for j, (node_id, node_ranges) in enumerate(ranges.items()):\n",
    "            feature_key = f\"x{feature}\"\n",
    "            axs[i].plot(node_ranges[feature], [i]*2, marker='o', color=colors[j], label=f\"Node {node_id} {feature_key}\")\n",
    "            axs[i].set_title(f\"Range of {feature_key} for class 1\")\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.legend()\n",
    "        ax.set_yticks([])  # Hide y ticks\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "features_of_interest = [17, 20, 13]\n",
    "node_ids = [14, 6]\n",
    "ranges = calculate_feature_ranges(best_model, X_train, y_train, node_ids, features_of_interest)\n",
    "plot_feature_ranges(ranges, features_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_feature_ranges(tree_model, X_train, y_train, node_ids, features):\n",
    "    ranges = {}\n",
    "    for node_id in node_ids:\n",
    "        leaf_ids = tree_model.apply(X_train)\n",
    "        samples_in_node = X_train.iloc[leaf_ids == node_id]\n",
    "        class_1_samples = samples_in_node[y_train.iloc[leaf_ids == node_id] == 1]\n",
    "        ranges[node_id] = {feature: (class_1_samples.iloc[:, feature].min(), class_1_samples.iloc[:, feature].max()) for feature in features}\n",
    "    return ranges\n",
    "\n",
    "def can_avoid_range(valid_range, avoid_range):\n",
    "    return not (valid_range[0] >= avoid_range[0] and valid_range[1] <= avoid_range[1])\n",
    "\n",
    "def sample_avoiding_range(valid_range, avoid_range):\n",
    "    if not can_avoid_range(valid_range, avoid_range):\n",
    "        return np.random.uniform(valid_range[0], valid_range[1])\n",
    "    \n",
    "    while True:\n",
    "        value = np.random.uniform(valid_range[0], valid_range[1])\n",
    "        if not (avoid_range[0] <= value <= avoid_range[1]):\n",
    "            return value\n",
    "\n",
    "def generate_avoiding_samples(node_14_data, node_14_ranges, node_6_ranges, num_samples):\n",
    "    generated_samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sample = {}\n",
    "        # Generate samples for the avoiding features\n",
    "        for feature in [13, 17, 20]:\n",
    "            valid_range = node_14_ranges[feature]\n",
    "            avoid_range = node_6_ranges[feature]\n",
    "            sample[feature] = sample_avoiding_range(valid_range, avoid_range)\n",
    "        \n",
    "        # Keep other features from the existing node 14 class 1 samples\n",
    "        other_features = node_14_data.drop(columns=[13, 17, 20]).sample(n=1, replace=True).iloc[0].to_dict()\n",
    "        sample.update(other_features)\n",
    "\n",
    "        generated_samples.append(sample)\n",
    "    \n",
    "    generated_samples_df = pd.DataFrame(generated_samples)\n",
    "    desired_order = node_14_data.columns.tolist()\n",
    "    generated_samples_df = generated_samples_df[desired_order]\n",
    "    generated_samples_df['label'] = 1\n",
    "\n",
    "    return generated_samples_df\n",
    "\n",
    "# Example usage\n",
    "features_of_interest = [13, 17, 20]\n",
    "node_ids = [14, 6]\n",
    "ranges = calculate_feature_ranges(best_model, X_train, y_train, node_ids, features_of_interest)\n",
    "\n",
    "# Extract node-specific data and ranges\n",
    "node_14_data = X_train[(best_model.apply(X_train) == 14) & (y_train == 1)]  # Class 1 samples from node 14\n",
    "node_14_ranges = ranges[14]\n",
    "node_6_ranges = ranges[6]\n",
    "\n",
    "num_class_1_samples_node_14 = len(node_14_data)\n",
    "generated_samples = generate_avoiding_samples(node_14_data, node_14_ranges, node_6_ranges, num_samples=num_class_1_samples_node_14)\n",
    "\n",
    "print(f\"Number of generated samples: {len(generated_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_generated_samples(tree_model, generated_samples):\n",
    "    # Drop the 'label' column if it exists in the DataFrame\n",
    "    samples_without_label = generated_samples.drop(columns=['label'], errors='ignore')\n",
    "\n",
    "    # Get the node assignments for the generated samples\n",
    "    node_assignments = tree_model.apply(samples_without_label)\n",
    "\n",
    "    # Get the predicted class labels for the generated samples\n",
    "    predicted_labels = tree_model.predict(samples_without_label)\n",
    "\n",
    "    # Combine the results into a DataFrame for easy analysis\n",
    "    observation_df = samples_without_label.copy()\n",
    "    observation_df['Node_Assignment'] = node_assignments\n",
    "    observation_df['Predicted_Label'] = predicted_labels\n",
    "\n",
    "    return observation_df\n",
    "\n",
    "# Pass the generated samples through the decision tree\n",
    "observation_results = observe_generated_samples(best_model, generated_samples)\n",
    "\n",
    "# Display the results\n",
    "print(\"Observation of Generated Samples:\")\n",
    "\n",
    "# Analyze the distribution of nodes and predicted classes\n",
    "node_distribution = observation_results['Node_Assignment'].value_counts()\n",
    "predicted_class_distribution = observation_results['Predicted_Label'].value_counts()\n",
    "\n",
    "print(\"\\nNode Distribution of Generated Samples:\")\n",
    "print(node_distribution)\n",
    "\n",
    "print(\"\\nPredicted Class Distribution of Generated Samples:\")\n",
    "print(predicted_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the initial decision tree with node accuracy\n",
    "print(\"\\nInitial Decision Tree Visualization with Node Accuracy:\")\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = [f\"x{index}\" for index in range(num_features)]\n",
    "visualize_tree_with_node_accuracy(best_model, node_accuracies, feature_names)\n",
    "\n",
    "# Combine the generated samples with the original training data\n",
    "X_train_augmented = pd.concat([X_train, generated_samples.drop(columns=['label'])], ignore_index=True)\n",
    "y_train_augmented = pd.concat([y_train, generated_samples['label']], ignore_index=True)\n",
    "\n",
    "# Perform grid search on the augmented data\n",
    "grid_search_augmented = perform_grid_search(X_train_augmented, y_train_augmented)\n",
    "print(f\"Best Parameters for Retrained Model: {grid_search_augmented.best_params_}\")\n",
    "\n",
    "# Retrain the model with the best parameters from grid search\n",
    "best_retrained_model = train_best_model(grid_search_augmented, X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Calculate node accuracy for the retrained model\n",
    "node_accuracies_retrained = calculate_node_accuracy(best_retrained_model, X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Plot the retrained decision tree with node accuracy\n",
    "print(\"\\nVisualization of Retrained Decision Tree with Node Accuracy:\")\n",
    "visualize_tree_with_node_accuracy(best_retrained_model, node_accuracies_retrained, feature_names)\n",
    "\n",
    "# Evaluate the retrained model on the test set\n",
    "print(\"\\nEvaluation of the Retrained Model:\")\n",
    "evaluate_model(best_retrained_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
